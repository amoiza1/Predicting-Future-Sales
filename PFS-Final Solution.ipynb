{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import gc\n",
    "from itertools import product\n",
    "\n",
    "pd.set_option('display.max_rows',99)\n",
    "pd.set_option('display.max_columns',50)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    float_cols = [col for col in df if df[col].dtype == 'float64']\n",
    "    int_cols = [col for col in df if df[col].dtype in ['int64','int32']]\n",
    "    \n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train = pd.read_csv('sales_train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train[sale_train['item_id'] == 11373][['item_price']].sort_values(['item_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train[sale_train['item_id'] == 11365].sort_values(['item_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train['item_price'][2909818] = np.nan\n",
    "sale_train['item_cnt_day'][2909818] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train['item_price'][2909818] = sale_train[(sale_train['shop_id'] ==12) & (sale_train['item_id'] == 11373) & (sale_train['date_block_num'] == 33)]['item_price'].median()\n",
    "sale_train['item_cnt_day'][2909818] = round(sale_train[(sale_train['shop_id'] ==12) & (sale_train['item_id'] == 11373) & (sale_train['date_block_num'] == 33)]['item_cnt_day'].median())\n",
    "sale_train['item_price'][885138] = np.nan\n",
    "sale_train['item_price'][885138] = sale_train[(sale_train['item_id'] == 11365) & (sale_train['shop_id'] ==12) & (sale_train['date_block_num'] == 8)]['item_price'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nrow = test.shape[0]\n",
    "sale_train = sale_train.merge(test[['shop_id']].drop_duplicates(), how = 'inner')\n",
    "sale_train['date'] = pd.to_datetime(sale_train['date'], format = '%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating grid\n",
    "grid = []\n",
    "index_cols = ['shop_id','item_id','date_block_num']\n",
    "\n",
    "for block in sale_train['date_block_num'].unique():\n",
    "    cur_shops = sale_train.loc[sale_train['date_block_num'] == block, 'shop_id'].unique()\n",
    "    cur_items = sale_train.loc[sale_train['date_block_num'] == block, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops,cur_items,[block]])),dtype=np.int32))\n",
    "\n",
    "grid = pd.DataFrame(np.vstack(grid),columns = index_cols, dtype=np.int32)\n",
    "grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_train['item_cnt_day']=sale_train['item_cnt_day'].clip(0,20)\n",
    "sale_train.item_cnt_day.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cnt = sale_train.groupby(index_cols)['item_cnt_day'].agg('sum').reset_index().rename(columns={'item_cnt_day':'item_cnt_month'})\n",
    "gb_cnt['item_cnt_month'] = gb_cnt['item_cnt_month'].clip(0,20).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cnt['item_cnt_month'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(grid,gb_cnt,how='left',on=index_cols).fillna(0)\n",
    "train['item_cnt_month'] = train['item_cnt_month'].astype(int)\n",
    "train = downcast_dtypes(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.item_cnt_month.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sort_values(['date_block_num','shop_id','item_id'],inplace = True)\n",
    "train                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = pd.read_csv('items.csv')\n",
    "train = train.merge(item[['item_id','item_category_id']],on='item_id',how='left')\n",
    "test = test.merge(item[['item_id','item_category_id']],on='item_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat = pd.read_csv('item_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cat = list(item_cat.item_category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0,1):\n",
    "\n",
    "    l_cat[ind] = 'PC Headsets / Headphones'\n",
    "\n",
    "for ind in range(1,8):\n",
    "\n",
    "    l_cat[ind] = 'Access'\n",
    "\n",
    "l_cat[8] = 'Tickets (figure)'\n",
    "\n",
    "l_cat[9] = 'Delivery of goods'\n",
    "\n",
    "for ind in range(10,18):\n",
    "\n",
    "    l_cat[ind] = 'Consoles'\n",
    "\n",
    "for ind in range(18,25):\n",
    "\n",
    "    l_cat[ind] = 'Consoles Games'\n",
    "\n",
    "l_cat[25] = 'Accessories for games'\n",
    "\n",
    "for ind in range(26,28):\n",
    "\n",
    "    l_cat[ind] = 'phone games'\n",
    "\n",
    "for ind in range(28,32):\n",
    "\n",
    "    l_cat[ind] = 'CD games'\n",
    "\n",
    "for ind in range(32,37):\n",
    "\n",
    "    l_cat[ind] = 'Card'\n",
    "\n",
    "for ind in range(37,43):\n",
    "\n",
    "    l_cat[ind] = 'Movie'\n",
    "\n",
    "for ind in range(43,55):\n",
    "\n",
    "    l_cat[ind] = 'Books'\n",
    "\n",
    "for ind in range(55,61):\n",
    "\n",
    "    l_cat[ind] = 'Music'\n",
    "\n",
    "for ind in range(61,73):\n",
    "\n",
    "    l_cat[ind] = 'Gifts'\n",
    "\n",
    "for ind in range(73,79):\n",
    "\n",
    "    l_cat[ind] = 'Soft'\n",
    "\n",
    "for ind in range(79,81):\n",
    "\n",
    "    l_cat[ind] = 'Office'\n",
    "\n",
    "for ind in range(81,83):\n",
    "\n",
    "    l_cat[ind] = 'Clean'\n",
    "\n",
    "l_cat[83] = 'Elements of a food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "item_cat['item_cat_id_fix'] = lb.fit_transform(l_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(item_cat[['item_category_id','item_cat_id_fix']],on='item_category_id',how='left')\n",
    "test = test.merge(item_cat[['item_category_id','item_cat_id_fix']],on='item_category_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(item, item_cat, grid, gb_cnt)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encodings\n",
    "Target = 'item_cnt_month'\n",
    "global_mean = train[Target].mean()\n",
    "y_tr = train[Target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoded_col = ['shop_id', 'item_id', 'item_category_id', 'item_cat_id_fix']\n",
    "SEED = 0\n",
    "\n",
    "for col in tqdm_notebook(mean_encoded_col):\n",
    "\n",
    "    col_tr = train[[col] + [Target]]\n",
    "   \n",
    "    corrcoefs = pd.DataFrame(columns = ['Cor'])\n",
    "\n",
    "\n",
    "\n",
    "    # 3.1.1 Mean encodings - KFold scheme\n",
    "\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    kf = KFold(n_splits = 5, shuffle = False, random_state = SEED)\n",
    "\n",
    "\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean_Kfold'] = global_mean\n",
    "\n",
    "    for tr_ind, val_ind in kf.split(col_tr):\n",
    "\n",
    "        X_tr, X_val = col_tr.iloc[tr_ind], col_tr.iloc[val_ind]\n",
    "\n",
    "        means = X_val[col].map(X_tr.groupby(col)[Target].mean())\n",
    "\n",
    "        X_val[col + '_cnt_month_mean_Kfold'] = means\n",
    "\n",
    "        col_tr.iloc[val_ind] = X_val\n",
    "\n",
    "        # X_val.head()\n",
    "\n",
    "    col_tr.fillna(global_mean, inplace = True)\n",
    "\n",
    "    corrcoefs.loc[col + '_cnt_month_mean_Kfold'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Kfold'])[0][1]\n",
    "\n",
    "\n",
    "\n",
    "    # 3.1.2 Mean encodings - Leave-one-out scheme\n",
    "\n",
    "    item_id_target_sum = col_tr.groupby(col)[Target].sum()\n",
    "\n",
    "    item_id_target_count = col_tr.groupby(col)[Target].count()\n",
    "\n",
    "    col_tr[col + '_cnt_month_sum'] = col_tr[col].map(item_id_target_sum)\n",
    "\n",
    "    col_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\n",
    "\n",
    "    col_tr[col + '_target_mean_LOO'] = (col_tr[col + '_cnt_month_sum'] - col_tr[Target]) / (col_tr[col + '_cnt_month_count'] - 1)\n",
    "\n",
    "    col_tr.fillna(global_mean, inplace = True)\n",
    "\n",
    "    corrcoefs.loc[col + '_target_mean_LOO'] = np.corrcoef(y_tr, col_tr[col + '_target_mean_LOO'])[0][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3.1.3 Mean encodings - Smoothing\n",
    "\n",
    "    item_id_target_mean = col_tr.groupby(col)[Target].mean()\n",
    "\n",
    "    item_id_target_count = col_tr.groupby(col)[Target].count()\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean'] = col_tr[col].map(item_id_target_mean)\n",
    "\n",
    "    col_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\n",
    "\n",
    "    alpha = 100\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean_Smooth'] = (col_tr[col + '_cnt_month_mean'] *  col_tr[col + '_cnt_month_count'] + global_mean * alpha) / (alpha + col_tr[col + '_cnt_month_count'])\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean_Smooth'].fillna(global_mean, inplace=True)\n",
    "\n",
    "    corrcoefs.loc[col + '_cnt_month_mean_Smooth'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Smooth'])[0][1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 3.1.4 Mean encodings - Expanding mean scheme\n",
    "\n",
    "    cumsum = col_tr.groupby(col)[Target].cumsum() - col_tr[Target]\n",
    "\n",
    "    sumcnt = col_tr.groupby(col).cumcount()\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean_Expanding'] = cumsum / sumcnt\n",
    "\n",
    "    col_tr[col + '_cnt_month_mean_Expanding'].fillna(global_mean, inplace=True)\n",
    "\n",
    "    corrcoefs.loc[col + '_cnt_month_mean_Expanding'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Expanding'])[0][1]\n",
    "\n",
    "\n",
    "\n",
    "    train = pd.concat([train, col_tr[corrcoefs['Cor'].idxmax()]], axis = 1)\n",
    "\n",
    "    print(corrcoefs.sort_values('Cor'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation = False\n",
    "train = train.loc[:,~train.columns.duplicated()]\n",
    "\n",
    "if Validation == False:\n",
    "\n",
    "    test['date_block_num'] = 34\n",
    "\n",
    "    all_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "    all_data = all_data.drop(columns = ['ID'])\n",
    "\n",
    "else:\n",
    "\n",
    "    all_data = train\n",
    "\n",
    "\n",
    "\n",
    "del train, test, col_tr\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "all_data = downcast_dtypes(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Lags\n",
    "index_cols = ['shop_id', 'item_id', 'item_category_id', 'item_cat_id_fix', 'date_block_num']\n",
    "\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols))\n",
    "\n",
    "print(cols_to_rename)\n",
    "\n",
    "shift_range = [1, 2, 3, 4, 12]\n",
    "\n",
    "\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "\n",
    "\n",
    "\n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "\n",
    "\n",
    "\n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "del train_shift\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[all_data['date_block_num'] >= 12] # Don't use old data from year 2013\n",
    "\n",
    "lag_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]]\n",
    "\n",
    "all_data = downcast_dtypes(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating date features --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = sale_train[['date', 'date_block_num']].drop_duplicates()\n",
    "\n",
    "dates_test = dates_train[dates_train['date_block_num'] == 34-12]\n",
    "dates_test['date_block_num'] = 34\n",
    "dates_test['date'] = dates_test['date'] + pd.DateOffset(years=1)\n",
    "dates_all = pd.concat([dates_train, dates_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_all['dow'] = dates_all['date'].dt.dayofweek\n",
    "dates_all['year'] = dates_all['date'].dt.year\n",
    "dates_all['month'] = dates_all['date'].dt.month\n",
    "dates_all = pd.get_dummies(dates_all, columns=['dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_col = ['dow_' + str(x) for x in range(7)]\n",
    "\n",
    "date_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\n",
    "\n",
    "date_features['days_of_month'] = date_features[dow_col].sum(axis=1)\n",
    "\n",
    "date_features['year'] = date_features['year'] - 2013\n",
    "\n",
    "\n",
    "\n",
    "date_features = date_features[['month', 'year', 'days_of_month', 'date_block_num']]\n",
    "\n",
    "all_data = all_data.merge(date_features, on = 'date_block_num', how = 'left')\n",
    "\n",
    "date_columns = date_features.columns.difference(set(index_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Scale feature columns --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train = all_data[all_data['date_block_num'] != all_data['date_block_num'].max()]\n",
    "\n",
    "test = all_data[all_data['date_block_num']== all_data['date_block_num'].max()]\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "to_drop_cols = ['date_block_num']\n",
    "\n",
    "feature_columns = list(set(lag_cols + index_cols + list(date_columns)).difference(to_drop_cols))\n",
    "\n",
    "\n",
    "\n",
    "train[feature_columns] = sc.fit_transform(train[feature_columns])\n",
    "\n",
    "test[feature_columns] = sc.transform(test[feature_columns])\n",
    "\n",
    "all_data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "all_data = downcast_dtypes(all_data)\n",
    "\n",
    "\n",
    "\n",
    "del train, test, date_features, sale_train\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. First-level model ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 34\n",
      "['item_cat_id_fix_cnt_month_mean_Expanding_lag_12', 'item_cat_id_fix_cnt_month_mean_Expanding_lag_4', 'item_cat_id_fix_cnt_month_mean_Expanding_lag_1', 'shop_id_cnt_month_mean_Expanding_lag_12', 'item_id', 'shop_id_cnt_month_mean_Expanding_lag_2', 'item_category_id_cnt_month_mean_Expanding_lag_1', 'item_id_cnt_month_mean_Expanding_lag_1', 'item_cnt_month_lag_4', 'item_cnt_month_lag_3', 'item_category_id_cnt_month_mean_Expanding_lag_4', 'item_id_cnt_month_mean_Expanding_lag_12', 'item_cat_id_fix', 'month', 'item_cnt_month_lag_12', 'shop_id_cnt_month_mean_Expanding_lag_3', 'shop_id_cnt_month_mean_Expanding_lag_4', 'shop_id', 'days_of_month', 'year', 'item_category_id_cnt_month_mean_Expanding_lag_3', 'shop_id_cnt_month_mean_Expanding_lag_1', 'item_cat_id_fix_cnt_month_mean_Expanding_lag_2', 'item_cnt_month_lag_2', 'item_cnt_month_lag_1', 'item_cat_id_fix_cnt_month_mean_Expanding_lag_3', 'item_id_cnt_month_mean_Expanding_lag_4', 'item_id_cnt_month_mean_Expanding_lag_3', 'item_category_id_cnt_month_mean_Expanding_lag_2', 'item_category_id_cnt_month_mean_Expanding_lag_12', 'item_category_id', 'item_id_cnt_month_mean_Expanding_lag_2']\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts\n",
    "\n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "\n",
    "print('Test `date_block_num` is %d' % last_block)\n",
    "\n",
    "print(feature_columns)\n",
    "\n",
    "#start_first_level_total = time.perf_counter()\n",
    "\n",
    "scoringMethod = 'r2'; from sklearn.metrics import mean_squared_error; from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train meta-features M = 15 (12 + 15 = 27)\n",
    "\n",
    "num_first_level_models = 3\n",
    "\n",
    "months_to_generate_meta_features = range(27,last_block +1)\n",
    "\n",
    "mask = dates.isin(months_to_generate_meta_features)\n",
    "\n",
    "Target = 'item_cnt_month'\n",
    "\n",
    "y_all_level2 = all_data[Target][mask].values\n",
    "\n",
    "X_all_level2 = np.zeros([y_all_level2.shape[0], num_first_level_models])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(27, 35)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now fill `X_train_level2` with metafeatures\n",
    "\n",
    "slice_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "              eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "              learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "              n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=0,\n",
       "              shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "              warm_start=False)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import (LinearRegression, SGDRegressor)\n",
    "import lightgbm as lgb\n",
    "\n",
    "sgdr= SGDRegressor(\n",
    "\n",
    "        penalty = 'l2' ,\n",
    "\n",
    "        random_state = SEED )\n",
    " \n",
    "estimators = [sgdr]\n",
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edae1f9df01d483ab672c7a942be735b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Start training for month 27\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 11.64 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 194.07 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "3746814/3746814 [==============================] - 4s 1us/step - loss: 1.2367 - mse: 1.2367\n",
      "Epoch 2/5\n",
      "3746814/3746814 [==============================] - 4s 1us/step - loss: 0.9493 - mse: 0.9493\n",
      "Epoch 3/5\n",
      "3746814/3746814 [==============================] - 4s 1us/step - loss: 0.9018 - mse: 0.9018\n",
      "Epoch 4/5\n",
      "3746814/3746814 [==============================] - 4s 1us/step - loss: 0.8861 - mse: 0.8861\n",
      "Epoch 5/5\n",
      "3746814/3746814 [==============================] - 4s 1us/step - loss: 0.8798 - mse: 0.8798\n",
      "221482/221482 [==============================] - 0s 0us/step\n",
      "lightgbm runs for 19.27 seconds.\n",
      "Total running time was 3.57 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 28\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 10.80 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 198.36 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "3968296/3968296 [==============================] - 4s 1us/step - loss: 1.2096 - mse: 1.2096\n",
      "Epoch 2/5\n",
      "3968296/3968296 [==============================] - 4s 1us/step - loss: 0.9370 - mse: 0.9370\n",
      "Epoch 3/5\n",
      "3968296/3968296 [==============================] - 4s 1us/step - loss: 0.8997 - mse: 0.8997\n",
      "Epoch 4/5\n",
      "3968296/3968296 [==============================] - 4s 1us/step - loss: 0.8868 - mse: 0.8868\n",
      "Epoch 5/5\n",
      "3968296/3968296 [==============================] - 4s 1us/step - loss: 0.8807 - mse: 0.8807\n",
      "212503/212503 [==============================] - 0s 0us/step\n",
      "lightgbm runs for 19.84 seconds.\n",
      "Total running time was 3.65 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 29\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 11.21 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 209.01 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4180799/4180799 [==============================] - 4s 1us/step - loss: 1.2137 - mse: 1.2137\n",
      "Epoch 2/5\n",
      "4180799/4180799 [==============================] - 4s 1us/step - loss: 0.9285 - mse: 0.9285\n",
      "Epoch 3/5\n",
      "4180799/4180799 [==============================] - 4s 1us/step - loss: 0.8857 - mse: 0.8857\n",
      "Epoch 4/5\n",
      "4180799/4180799 [==============================] - 4s 1us/step - loss: 0.8760 - mse: 0.8760\n",
      "Epoch 5/5\n",
      "4180799/4180799 [==============================] - 4s 1us/step - loss: 0.8710 - mse: 0.8710\n",
      "210494/210494 [==============================] - 0s 1us/step\n",
      "lightgbm runs for 20.91 seconds.\n",
      "Total running time was 3.85 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 30\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 12.49 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 220.82 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4391293/4391293 [==============================] - 4s 1us/step - loss: 1.1595 - mse: 1.1595\n",
      "Epoch 2/5\n",
      "4391293/4391293 [==============================] - 4s 1us/step - loss: 0.9035 - mse: 0.9035\n",
      "Epoch 3/5\n",
      "4391293/4391293 [==============================] - 4s 1us/step - loss: 0.8726 - mse: 0.8726\n",
      "Epoch 4/5\n",
      "4391293/4391293 [==============================] - 4s 1us/step - loss: 0.8642 - mse: 0.8642\n",
      "Epoch 5/5\n",
      "4391293/4391293 [==============================] - 4s 1us/step - loss: 0.8596 - mse: 0.8596\n",
      "215496/215496 [==============================] - 0s 1us/step\n",
      "lightgbm runs for 22.17 seconds.\n",
      "Total running time was 4.07 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 31\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 12.52 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 233.25 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4606789/4606789 [==============================] - 5s 1us/step - loss: 1.4276 - mse: 1.4276\n",
      "Epoch 2/5\n",
      "4606789/4606789 [==============================] - 5s 1us/step - loss: 0.8956 - mse: 0.8956\n",
      "Epoch 3/5\n",
      "4606789/4606789 [==============================] - 5s 1us/step - loss: 0.8543 - mse: 0.8543\n",
      "Epoch 4/5\n",
      "4606789/4606789 [==============================] - 5s 1us/step - loss: 0.8470 - mse: 0.8470\n",
      "Epoch 5/5\n",
      "4606789/4606789 [==============================] - 5s 1us/step - loss: 0.8422 - mse: 0.8422\n",
      "208444/208444 [==============================] - 0s 1us/step\n",
      "lightgbm runs for 24.85 seconds.\n",
      "Total running time was 4.32 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 32\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 13.57 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 242.67 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "4815233/4815233 [==============================] - 5s 1us/step - loss: 1.0962 - mse: 1.0962\n",
      "Epoch 2/5\n",
      "4815233/4815233 [==============================] - 5s 1us/step - loss: 0.8758 - mse: 0.8758\n",
      "Epoch 3/5\n",
      "4815233/4815233 [==============================] - 5s 1us/step - loss: 0.8507 - mse: 0.8507\n",
      "Epoch 4/5\n",
      "4815233/4815233 [==============================] - 5s 1us/step - loss: 0.8429 - mse: 0.8429\n",
      "Epoch 5/5\n",
      "4815233/4815233 [==============================] - 5s 1us/step - loss: 0.8387 - mse: 0.8387\n",
      "208075/208075 [==============================] - 0s 1us/step\n",
      "lightgbm runs for 25.49 seconds.\n",
      "Total running time was 4.49 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 33\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 14.01 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 249.03 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "5023308/5023308 [==============================] - 5s 1us/step - loss: 1.1253 - mse: 1.1253\n",
      "Epoch 2/5\n",
      "5023308/5023308 [==============================] - 6s 1us/step - loss: 0.8805 - mse: 0.8805\n",
      "Epoch 3/5\n",
      "5023308/5023308 [==============================] - 5s 1us/step - loss: 0.8503 - mse: 0.8503\n",
      "Epoch 4/5\n",
      "5023308/5023308 [==============================] - 5s 1us/step - loss: 0.8425 - mse: 0.8425\n",
      "Epoch 5/5\n",
      "5023308/5023308 [==============================] - 5s 1us/step - loss: 0.8377 - mse: 0.8377\n",
      "221802/221802 [==============================] - 0s 1us/step\n",
      "lightgbm runs for 27.01 seconds.\n",
      "Total running time was 4.62 minutes.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Start training for month 34\n",
      "Training Model 0: SGDRegressor\n",
      "SGDRegressor runs for 14.32 seconds.\n",
      "\n",
      "Training Model 1: lightgbm\n",
      "lightgbm runs for 260.33 seconds.\n",
      "\n",
      "Training Model 2: keras\n",
      "Epoch 1/5\n",
      "5245110/5245110 [==============================] - 6s 1us/step - loss: 1.5253 - mse: 1.5253\n",
      "Epoch 2/5\n",
      "5245110/5245110 [==============================] - 6s 1us/step - loss: 1.5253 - mse: 1.5253\n",
      "Epoch 3/5\n",
      "5245110/5245110 [==============================] - 6s 1us/step - loss: 1.5253 - mse: 1.5253\n",
      "Epoch 4/5\n",
      "5245110/5245110 [==============================] - 5s 1us/step - loss: 1.5253 - mse: 1.5253\n",
      "Epoch 5/5\n",
      "5245110/5245110 [==============================] - 6s 1us/step - loss: 1.5253 - mse: 1.5253\n",
      "214200/214200 [==============================] - 0s 1us/step\n",
      "lightgbm runs for 28.92 seconds.\n",
      "Total running time was 4.84 minutes.\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cur_block_num in tqdm_notebook(months_to_generate_meta_features):\n",
    "\n",
    "    print('-' * 50)\n",
    "\n",
    "    print('Start training for month %d'% cur_block_num)\n",
    "    \n",
    "    start_cur_month = time.perf_counter()\n",
    "\n",
    "    cur_X_train = all_data.loc[dates <  cur_block_num][feature_columns]\n",
    "\n",
    "    cur_X_test =  all_data.loc[dates == cur_block_num][feature_columns]\n",
    "\n",
    "\n",
    "    cur_y_train = all_data.loc[dates <  cur_block_num, Target].values\n",
    "\n",
    "    cur_y_test =  all_data.loc[dates == cur_block_num, Target].values\n",
    "\n",
    "\n",
    "\n",
    "    # Create Numpy arrays of train, test and target dataframes to feed into models\n",
    "\n",
    "    train_x = cur_X_train.values\n",
    "\n",
    "    train_y = cur_y_train.ravel()\n",
    "\n",
    "    test_x = cur_X_test.values\n",
    "\n",
    "    test_y = cur_y_test.ravel()\n",
    "\n",
    "\n",
    "\n",
    "    preds = []\n",
    "\n",
    "\n",
    "    #Modeling\n",
    "\n",
    "\n",
    "\n",
    "    sgdr= SGDRegressor(\n",
    "\n",
    "        penalty = 'l2' ,\n",
    "\n",
    "        random_state = SEED )\n",
    "\n",
    "    lgb_params = {\n",
    "\n",
    "                  'feature_fraction': 0.75,\n",
    "\n",
    "                  'metric': 'rmse',\n",
    "\n",
    "                  'nthread':1,\n",
    "\n",
    "                  'min_data_in_leaf': 2**7,\n",
    "\n",
    "                  'bagging_fraction': 0.75,\n",
    "\n",
    "                  'learning_rate': 0.03,\n",
    "\n",
    "                  'objective': 'mse',\n",
    "\n",
    "                  'bagging_seed': 2**7,\n",
    "\n",
    "                  'num_leaves': 2**7,\n",
    "\n",
    "                  'bagging_freq':1,\n",
    "\n",
    "                  'verbose':0\n",
    "\n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "    estimators = [sgdr]\n",
    "\n",
    "\n",
    "\n",
    "    for estimator in estimators:\n",
    "\n",
    "        print('Training Model %d: %s'%(len(preds), estimator.__class__.__name__))\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        estimator.fit(train_x, train_y)\n",
    "\n",
    "        pred_test = estimator.predict(test_x)\n",
    "\n",
    "        preds.append(pred_test)\n",
    "\n",
    "        # pred_train = estimator.predict(train_x)\n",
    "\n",
    "        # print('Train RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_train, pred_train))))\n",
    "\n",
    "        # print('Test RMSE for %s is %f' % (estimator.__class__.__name__, sqrt(mean_squared_error(cur_y_test, pred_test))))\n",
    "\n",
    "        run = time.perf_counter() - start\n",
    "\n",
    "        print('{} runs for {:.2f} seconds.'.format(estimator.__class__.__name__, run))\n",
    "\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Training Model %d: %s'%(len(preds), 'lightgbm'))\n",
    "\n",
    "    #start = time.perf_counter()\n",
    "\n",
    "    estimator = lgb.train(lgb_params, lgb.Dataset(train_x, label=train_y), 300)\n",
    "\n",
    "    pred_test = estimator.predict(test_x)\n",
    "\n",
    "    preds.append(pred_test)\n",
    "\n",
    "    # pred_train = estimator.predict(train_x)\n",
    "\n",
    "    # print('Train RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_train, pred_train))))\n",
    "\n",
    "    # print('Test RMSE for %s is %f' % ('lightgbm', sqrt(mean_squared_error(cur_y_test, pred_test))))\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    print('{} runs for {:.2f} seconds.'.format('lightgbm', run))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Training Model %d: %s'%(len(preds), 'keras'))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "\n",
    "    def baseline_model():\n",
    "\n",
    "    \t# create model\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(20, input_dim=train_x.shape[1], kernel_initializer='uniform', activation='softplus'))\n",
    "\n",
    "        model.add(Dense(1, kernel_initializer='uniform', activation = 'relu'))\n",
    "\n",
    "        # Compile model\n",
    "\n",
    "        model.compile(loss='mse', optimizer='Nadam', metrics=['mse'])\n",
    "\n",
    "        # model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    estimator = KerasRegressor(build_fn=baseline_model, verbose=1, epochs=5, batch_size = 55000)\n",
    "\n",
    "\n",
    "\n",
    "    estimator.fit(train_x, train_y)\n",
    "\n",
    "    pred_test = estimator.predict(test_x)\n",
    "\n",
    "    preds.append(pred_test)\n",
    "\n",
    "\n",
    "\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    print('{} runs for {:.2f} seconds.'.format('lightgbm', run))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cur_month_run_total = time.perf_counter() - start_cur_month\n",
    "\n",
    "    print('Total running time was {:.2f} minutes.'.format(cur_month_run_total/60))\n",
    "\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "\n",
    "    slice_end = slice_start + cur_X_test.shape[0]\n",
    "\n",
    "    X_all_level2[ slice_start : slice_end , :] = np.c_[preds].transpose()\n",
    "\n",
    "    slice_start = slice_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "\n",
    "test_nrow = len(preds[0])\n",
    "\n",
    "X_train_level2 = X_all_level2[ : -test_nrow, :]\n",
    "\n",
    "X_test_level2 = X_all_level2[ -test_nrow: , :]\n",
    "\n",
    "y_train_level2 = y_all_level2[ : -test_nrow]\n",
    "\n",
    "y_test_level2 = y_all_level2[ -test_nrow : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Second level learning model via linear regression\n",
      "Train RMSE for train_preds_lr_stacking is 0.815870\n"
     ]
    }
   ],
   "source": [
    "# 4. Ensembling -------------------------------------------------------------------\n",
    "\n",
    "pred_list = {}\n",
    "\n",
    "\n",
    "\n",
    "# A. Second level learning model via linear regression\n",
    "\n",
    "print('Training Second level learning model via linear regression')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import (LinearRegression, SGDRegressor)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train_level2, y_train_level2)\n",
    "\n",
    "# Compute R-squared on the train and test sets.\n",
    "\n",
    "# print('Train R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, lr.predict(X_train_level2)))))\n",
    "\n",
    "test_preds_lr_stacking = lr.predict(X_test_level2)\n",
    "\n",
    "train_preds_lr_stacking = lr.predict(X_train_level2)\n",
    "\n",
    "print('Train RMSE for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_lr_stacking))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Second level learning model via SGDRegressor\n"
     ]
    }
   ],
   "source": [
    "pred_list['test_preds_lr_stacking'] = test_preds_lr_stacking\n",
    "\n",
    "if Validation:\n",
    "\n",
    "    print('Test R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_test_level2, test_preds_lr_stacking))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# B. Second level learning model via SGDRegressor\n",
    "\n",
    "print('Training Second level learning model via SGDRegressor')\n",
    "\n",
    "sgdr= SGDRegressor(\n",
    "\n",
    "    penalty = 'l2' ,\n",
    "\n",
    "    random_state = SEED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for train_preds_lr_stacking is 0.818250\n"
     ]
    }
   ],
   "source": [
    "sgdr.fit(X_train_level2, y_train_level2)\n",
    "\n",
    "# Compute R-squared on the train and test sets.\n",
    "\n",
    "# print('Train R-squared for %s is %f' %('test_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, lr.predict(X_train_level2)))))\n",
    "\n",
    "test_preds_sgdr_stacking = sgdr.predict(X_test_level2)\n",
    "\n",
    "train_preds_sgdr_stacking = sgdr.predict(X_train_level2)\n",
    "\n",
    "print('Train R-squared for %s is %f' %('train_preds_lr_stacking', sqrt(mean_squared_error(y_train_level2, train_preds_sgdr_stacking))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list['test_preds_sgdr_stacking'] = test_preds_sgdr_stacking\n",
    "\n",
    "if Validation:\n",
    "\n",
    "    print('Test R-squared for %s is %f' %('test_preds_sgdr_stacking', sqrt(mean_squared_error(y_test_level2, test_preds_sgdr_stacking))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_preds_sgdr_stacking': array([0.50737028, 0.30767959, 0.82742931, ..., 0.04457132, 0.02347315,\n",
       "        0.04805526]),\n",
       " 'test_preds_lr_stacking': array([0.54395551, 0.31808356, 0.90842739, ..., 0.04801198, 0.02955192,\n",
       "        0.05232132])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28339887459977325\n",
      "0.2613999374887801\n"
     ]
    }
   ],
   "source": [
    "if not Validation:\n",
    "\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "\n",
    "\n",
    "    ver = 6\n",
    "\n",
    "    for pred_ver in ['lr_stacking', 'sgdr_stacking']:\n",
    "\n",
    "        print(pred_list['test_preds_' + pred_ver].clip(0,20).mean())\n",
    "\n",
    "        submission['item_cnt_month'] = pred_list['test_preds_' + pred_ver].clip(0,20)\n",
    "\n",
    "        submission[['ID', 'item_cnt_month']].to_csv('%d_%s.csv' % (ver, pred_ver), index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
